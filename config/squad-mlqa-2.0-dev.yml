max_submissions_per_period: 5 # allows at most 5 submissions per user per period, where period is 24 hours by default
log_worksheet_uuid: "0xcea4709fe6c8485a8228e9d36f8f5cde" # uuid of the worksheet to create new run bundles in
submission_tag: squad-test-submit # configure the tag that participants use to submit to the competition

mimic_datasets:
  new: "0xd2ad1dcaa85f4d6f92d68b44fc20e9d8"
  old: "0x8f29fe78ffe545128caccab74eb06c57"

# Configure how to mimic the submitted prediction bundles
predict:
  metadata:
    request_memory: 12G
    request_gpus: 1
    request_cpus: 2
    request_queue: tag=nibel-worker
  mimic:
    - {
        new: "0xd2ad1dcaa85f4d6f92d68b44fc20e9d8",
        old: "0x8f29fe78ffe545128caccab74eb06c57",
      } # replace `old` bundle with `new` bundle
  tag: leaderboard-v2.0-mlqa-dev-predict

## Configure how to evaluate the new prediction bundles
evaluate:
  dependencies:
    - {
        child_path: evaluate.py,
        parent_uuid: "0xbcd57bee090b421c982906709c8c27e1",
      }
    - {
        child_path: test.json,
        parent_uuid: "0x9ee64ab986174e62a23cbe54a2b875fd",
      }
    - { child_path: predictions.json, parent_uuid: "{predict}" }
  command: python evaluate.py test.json predictions.json
  tag: leaderboard-v2.0-mlqa-dev-predict

# Define how to extract the scores from the evaluation bundle
score_specs:
  - { key: "/stdout:f1", name: f1 }
  - { key: "/stdout:exact_match", name: exact_match }
